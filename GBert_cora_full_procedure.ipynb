{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "24d99aaa",
   "metadata": {},
   "source": [
    "# Load Cora Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "32ebef20",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/Python_interpre/lib/python3.12/site-packages/torch_geometric/typing.py:124: UserWarning: An issue occurred while importing 'torch-sparse'. Disabling its usage. Stacktrace: dlopen(/opt/anaconda3/envs/Python_interpre/lib/python3.12/site-packages/torch_sparse/_convert_cpu.so, 0x0006): Symbol not found: __ZN2at8internal15invoke_parallelExxxRKNSt3__18functionIFvxxEEE\n",
      "  Referenced from: <10480BD5-33A4-3A5E-8C3D-8961DBA73F4E> /opt/anaconda3/envs/Python_interpre/lib/python3.12/site-packages/torch_sparse/_convert_cpu.so\n",
      "  Expected in:     <616791F0-29C3-3F88-8A88-D072E7E40979> /opt/anaconda3/envs/Python_interpre/lib/libtorch_cpu.dylib\n",
      "  warnings.warn(f\"An issue occurred while importing 'torch-sparse'. \"\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "content_path = './data/Cora/cora.content'\n",
    "cites_path = './data/Cora/cora.cites'\n",
    "\n",
    "content_df = pd.read_csv(content_path, sep='\\t', header=None)\n",
    "\n",
    "paper_ids = content_df[0].tolist()  \n",
    "features = torch.tensor(content_df.iloc[:, 1:-1].values, dtype=torch.float) \n",
    "labels_raw = content_df.iloc[:, -1].tolist()  \n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "labels = torch.tensor(label_encoder.fit_transform(labels_raw), dtype=torch.long)\n",
    "\n",
    "id_map = {pid: i for i, pid in enumerate(paper_ids)}\n",
    "\n",
    "cites_df = pd.read_csv(cites_path, sep='\\t', header=None, names=['source', 'target'])\n",
    "\n",
    "cites_df = cites_df[cites_df['source'].isin(id_map) & cites_df['target'].isin(id_map)]\n",
    "\n",
    "src = cites_df['source'].map(id_map).tolist()\n",
    "dst = cites_df['target'].map(id_map).tolist()\n",
    "\n",
    "edge_index = torch.tensor([src, dst], dtype=torch.long)\n",
    "\n",
    "data_raw = Data(x=features, edge_index=edge_index, y=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f9968651",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data_raw # data = data_preprocess\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "import torch.nn.functional as F\n",
    "\n",
    "########## one-hot encoding\n",
    "num_classes = len(data.y.unique())  # Cora has 7 classes\n",
    "\n",
    "# F.one_hot for one-hot coding\n",
    "y_one_hot = F.one_hot(data.y, num_classes=num_classes).float()  # cora dataset shape: [2708, 7] or ABIDE dataset shape: [270000, 2]\n",
    "\n",
    "data.num_nodes = data.x.shape[0]\n",
    "data.node_list = list(range(data.num_nodes))  # initial nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e77c375",
   "metadata": {},
   "source": [
    "# intimacy calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "99276cde",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/18/jlhdr74n64dfb6pm21lq8x180000gn/T/ipykernel_77984/606168343.py:33: UserWarning: torch.sparse.SparseTensor(indices, values, shape, *, device=) is deprecated.  Please use torch.sparse_coo_tensor(indices, values, shape, dtype=, device=). (Triggered internally at /private/var/folders/k1/30mswbxs7r1g6zwn8y4fyt500000gp/T/abs_e3pikzc5fh/croot/libtorch_1738337599132/work/torch/csrc/utils/tensor_new.cpp:653.)\n",
      "  return torch.sparse.FloatTensor(indices, values, shape)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import scipy.sparse as sp\n",
    "from numpy.linalg import inv\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "########## one-hot encoding\n",
    "num_classes = len(data.y.unique())  # Cora has 7 classes\n",
    "\n",
    "# F.one_hot for one-hot coding\n",
    "y_one_hot = F.one_hot(data.y, num_classes=num_classes).float()  # cora dataset shape: [2708, 7] or ABIDE dataset shape: [270000, 2]\n",
    "\n",
    "\n",
    "# building the adjacency matrix\n",
    "def adj_normalize(mx):\n",
    "    \"\"\"Row-normalize sparse matrix\"\"\"\n",
    "    rowsum = np.array(mx.sum(1))  # Sum over rows (degrees of nodes)\n",
    "    r_inv = np.power(rowsum, -0.5).flatten()  # Inverse square root of degrees\n",
    "    r_inv[np.isinf(r_inv)] = 0.  # Handle division by zero (e.g., for isolated nodes)\n",
    "    r_mat_inv = sp.diags(r_inv)  # Create a sparse diagonal matrix from the inverse degrees\n",
    "    mx = r_mat_inv.dot(mx).dot(r_mat_inv)  # Apply the normalization formula\n",
    "    return mx\n",
    "\n",
    "\n",
    "def sparse_mx_to_torch_sparse_tensor(sparse_mx):\n",
    "    \"\"\"Convert a scipy sparse matrix to a torch sparse tensor.\"\"\"\n",
    "    sparse_mx = sparse_mx.tocoo().astype(np.float32)\n",
    "    indices = torch.from_numpy(\n",
    "        np.vstack((sparse_mx.row, sparse_mx.col)).astype(np.int64))\n",
    "    values = torch.from_numpy(sparse_mx.data)\n",
    "    shape = torch.Size(sparse_mx.shape)\n",
    "    return torch.sparse.FloatTensor(indices, values, shape)\n",
    "\n",
    "\n",
    "edges_index = data.edge_index.cpu().numpy().T  # [num_edges, 2]\n",
    "adj = sp.coo_matrix(\n",
    "    (np.ones(edges_index.shape[0]), (edges_index[:, 0], edges_index[:, 1])),\n",
    "    shape=(y_one_hot.shape[0], y_one_hot.shape[0]),\n",
    "    dtype=np.float32\n",
    ")\n",
    "adj = adj + adj.T.multiply(adj.T > adj) - adj.multiply(adj.T > adj)\n",
    "\n",
    "norm_adj = adj_normalize(adj + sp.eye(adj.shape[0]))# normalized adjacency matrix\n",
    "\n",
    "data.adj = sparse_mx_to_torch_sparse_tensor(norm_adj)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b0fbdbd",
   "metadata": {},
   "source": [
    "# Weisfeiler-Lehman (WL) based graph coloring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ac69e279",
   "metadata": {},
   "outputs": [],
   "source": [
    "import hashlib\n",
    "import pickle\n",
    "import os\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class WLGraphColoring:\n",
    "    def __init__(self, max_iter=2):\n",
    "        self.max_iter = max_iter\n",
    "        self.node_color_dict = {}\n",
    "        self.node_neighbor_dict = {}\n",
    "\n",
    "    def setting_init(self, node_list, edge_index):\n",
    "        for node in node_list:\n",
    "            self.node_color_dict[node] = 1\n",
    "            self.node_neighbor_dict[node] = {}\n",
    "\n",
    "        for i in range(edge_index.shape[1]):\n",
    "            u1, u2 = edge_index[0, i].item(), edge_index[1, i].item()\n",
    "            self.node_neighbor_dict[u1][u2] = 1\n",
    "            self.node_neighbor_dict[u2][u1] = 1\n",
    "\n",
    "    def WL_recursion(self, node_list):\n",
    "        iteration_count = 1\n",
    "        while True:\n",
    "            new_color_dict = {}\n",
    "            for node in node_list:\n",
    "                neighbors = self.node_neighbor_dict[node]\n",
    "                neighbor_color_list = [self.node_color_dict[neb] for neb in neighbors]\n",
    "                color_string_list = [str(self.node_color_dict[node])] + sorted([str(color) for color in neighbor_color_list])\n",
    "                color_string = \"_\".join(color_string_list)\n",
    "                hash_object = hashlib.md5(color_string.encode())\n",
    "                hashing = hash_object.hexdigest()  # Using MD5 hash function\n",
    "                new_color_dict[node] = hashing\n",
    "\n",
    "            color_index_dict = {k: v + 1 for v, k in enumerate(sorted(set(new_color_dict.values())))}\n",
    "            for node in new_color_dict:\n",
    "                new_color_dict[node] = color_index_dict[new_color_dict[node]]\n",
    "\n",
    "            if self.node_color_dict == new_color_dict or iteration_count == self.max_iter:\n",
    "                return  \n",
    "            else:\n",
    "                self.node_color_dict = new_color_dict\n",
    "            iteration_count += 1\n",
    "\n",
    "    def save_coloring(self, save_path):\n",
    "        os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
    "        with open(save_path, 'wb') as f:\n",
    "            pickle.dump(self.node_color_dict, f)\n",
    "\n",
    "    def visualize_graph(self):\n",
    "        G = nx.Graph()\n",
    "        for node, neighbors in self.node_neighbor_dict.items():\n",
    "            for neighbor in neighbors:\n",
    "                G.add_edge(node, neighbor)\n",
    "\n",
    "        node_colors = [self.node_color_dict[node] for node in G.nodes()]\n",
    "        plt.figure(figsize=(8, 8))\n",
    "        nx.draw(G, node_color=node_colors, with_labels=True, cmap=plt.cm.viridis)\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "# Usage example\n",
    "wl_coloring = WLGraphColoring()\n",
    "\n",
    "# Assuming `data.node_list` and `data.edge_index` are your node list and edge index\n",
    "wl_coloring.setting_init(data.node_list, data.edge_index)\n",
    "wl_coloring.WL_recursion(data.node_list)\n",
    "\n",
    "# Save the node color dict\n",
    "saving_path = './results'\n",
    "os.makedirs(os.path.dirname(f\"{saving_path}/WL/WL\"), exist_ok=True)\n",
    "\n",
    "wl_coloring.save_coloring(f'{saving_path}/WL/WL')\n",
    "\n",
    "# Visualize the graph coloring\n",
    "# wl_coloring.visualize_graph()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9278a0e5",
   "metadata": {},
   "source": [
    "# Top-k Personalized PageRank neighbor for propogation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9b77e927",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.sparse as sp\n",
    "import numpy as np\n",
    "\n",
    "import networkx as nx\n",
    "import scipy.sparse as sp\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "# PageRank-inspired adjacent matrix for propogation\n",
    "def neumann_approx_inverse(adj, c=0.15, K=10):\n",
    "    \"\"\"\n",
    "    c = 0.15\n",
    "    Neumann series approximation for eigen_adj = 0.15 * inv(I - (1 - 0.15) * A)\n",
    "    A is the normalized adjacency matrix\n",
    "    Returns a sparse matrix\n",
    "    \"\"\"\n",
    "    alpha = 1 - c\n",
    "    A = adj_normalize(adj)\n",
    "    n = A.shape[0]\n",
    "\n",
    "    # initial value is ones matrix\n",
    "    result = sp.eye(n, format='csr', dtype=np.float32)\n",
    "    A_power = sp.eye(n, format='csr', dtype=np.float32)  # A^0\n",
    "\n",
    "    for k in range(1, K + 1):\n",
    "        A_power = alpha * A.dot(A_power)  # A^k\n",
    "        result += A_power\n",
    "\n",
    "    return c * result  # c * sum(alpha^k A^k)\n",
    "\n",
    "\n",
    "#Sparse version\n",
    "# def get_top_k_sparse(eigen_adj: sp.spmatrix, k: int):\n",
    "def get_top_k_sparse(adj: sp.spmatrix, c: float, k: int):\n",
    "    \"\"\"\n",
    "    eigen_adj: scipy.sparse.csr_matrix or coo_matrix\n",
    "    k: int\n",
    "    return: dict[node] = [(neighbor, score), ...]\n",
    "    \"\"\"\n",
    "    \n",
    "    # PageRank-inspired adjacent matrix for propogation\n",
    "    eigen_adj = neumann_approx_inverse(adj, c=c, K=15)\n",
    "    \n",
    "    dense = eigen_adj.toarray()\n",
    "    n = dense.shape[0]\n",
    "    result_dict = {}\n",
    "    \n",
    "    key_map = None\n",
    "    for i in range(n):\n",
    "        scores = dense[i]\n",
    "        scores[i] = -np.inf # remove self connection\n",
    "        if k < n:\n",
    "            top_k_idx = np.argpartition(-scores, k)[:k]\n",
    "        else:\n",
    "            top_k_idx = np.arange(n)\n",
    "            top_k_idx = top_k_idx[top_k_idx != i]\n",
    "        \n",
    "        # sorting\n",
    "        top_k_idx = top_k_idx[np.argsort(-scores[top_k_idx])]\n",
    "\n",
    "        neighbors = [(idx, scores[idx]) for idx in top_k_idx]\n",
    "        mapped_node = key_map.get(i, i) if key_map else i\n",
    "        mapped_neighbors = [(key_map.get(nid, nid) if key_map else nid, val) for nid, val in neighbors]\n",
    "\n",
    "        result_dict[mapped_node] = mapped_neighbors\n",
    "\n",
    "        \n",
    "\n",
    "    return result_dict\n",
    "\n",
    "\n",
    "# KBatch = get_top_k_sparse(adj, 0.15, 2)# Test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ff9d902a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import networkx as nx\n",
    "import scipy.sparse as sp\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "import os\n",
    "\n",
    "def BatchHopDistance(node_list, edge_index, k, batch_path):\n",
    "    import pickle\n",
    "    import networkx as nx\n",
    "    \n",
    "    edge_index = edge_index.cpu().numpy()\n",
    "    link_list = list(zip(edge_index[0], edge_index[1]))\n",
    "\n",
    "    G = nx.Graph()\n",
    "    G.add_nodes_from(node_list)\n",
    "    G.add_edges_from(link_list)\n",
    "\n",
    "        \n",
    "\n",
    "    with open(f\"{batch_path}/Batch/top_{k}_GraphBatching\", 'rb') as f:\n",
    "        batch_dict = pickle.load(f)\n",
    "\n",
    "    hop_dict = {}\n",
    "\n",
    "    for node in batch_dict:\n",
    "        try:\n",
    "            node_hop_lengths = nx.single_source_shortest_path_length(G, node, cutoff=10)\n",
    "        except:\n",
    "            node_hop_lengths = {}\n",
    "\n",
    "        hop_dict[node] = {}\n",
    "        for neighbor, _ in batch_dict[node]:\n",
    "            hop = node_hop_lengths.get(neighbor, 99)\n",
    "            hop_dict[node][neighbor] = hop\n",
    "\n",
    "    return hop_dict\n",
    "\n",
    "##--------------------------------------------------------\n",
    "# you can define how many neighbors \n",
    "# for k in [1,2,3,4,5,6,7,8]:\n",
    "# for k in [40,50,60,70,80,90,100,110, 120, 130, 140, 150]:\n",
    "for k in [7]:\n",
    "    \n",
    "    # KBatch = get_top_k_sparse(adj, 0.15, k)\n",
    "    KBatch = get_top_k_sparse(adj, 0.15, k)\n",
    "    \n",
    "    os.makedirs(os.path.dirname(f\"{saving_path}/Batch/top_{k}_GraphBatching\"), exist_ok=True)\n",
    "    f = open(f\"{saving_path}/Batch/top_{k}_GraphBatching\", 'wb')\n",
    "    pickle.dump(KBatch, f)\n",
    "    f.close()\n",
    "    \n",
    "    KHop = BatchHopDistance(data.node_list, data.edge_index, k, saving_path)\n",
    "    os.makedirs(os.path.dirname(f\"{saving_path}/Hop/top_{k}_GraphBatchingHop\"), exist_ok=True)\n",
    "    f = open(f\"{saving_path}/Hop/top_{k}_GraphBatchingHop\", 'wb')\n",
    "    pickle.dump(KHop, f)\n",
    "    f.close()\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75b82f37",
   "metadata": {},
   "source": [
    "# Create embedding for the LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c3ae4c96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load WL Dictionary\n",
      "Load Hop Distance Dictionary\n",
      "Load Subgraph Batches\n"
     ]
    }
   ],
   "source": [
    "import hashlib\n",
    "import pickle\n",
    "\n",
    "def load_hop_wl_batch(save_dir,k):\n",
    "    print('Load WL Dictionary')\n",
    "    f = open(f'{save_dir}/WL/WL', 'rb')\n",
    "    # f = open(f'{save_dir}/WL/cora', 'rb')\n",
    "    wl_dict = pickle.load(f)\n",
    "    f.close()\n",
    "\n",
    "    print('Load Hop Distance Dictionary')\n",
    "    f = open(f'{save_dir}/Hop/top_{k}_GraphBatchingHop', 'rb')\n",
    "    # f = open(f'{save_dir}/Hop/hop_cora_{k}', 'rb')\n",
    "    hop_dict = pickle.load(f)\n",
    "    f.close()\n",
    "\n",
    "    print('Load Subgraph Batches')\n",
    "    f = open(f'{save_dir}/Batch/top_{k}_GraphBatching', 'rb')\n",
    "    # f = open(f'{save_dir}/Batch/cora_{k}', 'rb')\n",
    "    batch_dict = pickle.load(f)\n",
    "    f.close()\n",
    "\n",
    "    return hop_dict, wl_dict, batch_dict\n",
    "\n",
    "# Main functions\n",
    "embedding_dimension = 7\n",
    "saving_path = './results'\n",
    "hop_dict, wl_dict, batch_dict = load_hop_wl_batch(saving_path,embedding_dimension)\n",
    "\n",
    "# adj = sparse_mx_to_torch_sparse_tensor(norm_adj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "edf20039",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_feature_list = []\n",
    "role_ids_list = []\n",
    "position_ids_list = []\n",
    "hop_ids_list = []\n",
    "idx = data.node_list\n",
    "\n",
    "for node in idx:\n",
    "    node_index = node \n",
    "    neighbors_list = batch_dict[node]\n",
    "\n",
    "    raw_feature = [data.x[node_index].tolist()]\n",
    "    role_ids = [wl_dict[node]]\n",
    "    position_ids = range(len(neighbors_list) + 1)\n",
    "    hop_ids = [0]\n",
    "    for neighbor, intimacy_score in neighbors_list:\n",
    "        neighbor_index = neighbor\n",
    "        \n",
    "        raw_feature.append(data.x[neighbor_index].tolist())\n",
    "        role_ids.append(wl_dict[neighbor])\n",
    "        if neighbor in hop_dict[node]:\n",
    "            hop_ids.append(hop_dict[node][neighbor])\n",
    "        else:\n",
    "            hop_ids.append(99)\n",
    "    raw_feature_list.append(raw_feature)\n",
    "    role_ids_list.append(role_ids)\n",
    "    position_ids_list.append(position_ids)\n",
    "    hop_ids_list.append(hop_ids)\n",
    "\n",
    "raw_embeddings = torch.FloatTensor(raw_feature_list)\n",
    "wl_embedding = torch.LongTensor(role_ids_list)\n",
    "hop_embeddings = torch.LongTensor(hop_ids_list)\n",
    "int_embeddings = torch.LongTensor(position_ids_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aedb211",
   "metadata": {},
   "source": [
    "# Bert embedding and encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d82a2ae8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:torchao.kernel.intmm:Warning: Detected no triton, on systems without Triton certain kernels will not work\n",
      "/opt/anaconda3/envs/Python_interpre/lib/python3.12/site-packages/torchvision/io/image.py:14: UserWarning: Failed to load image Python extension: 'dlopen(/opt/anaconda3/envs/Python_interpre/lib/python3.12/site-packages/torchvision/image.so, 0x0006): Library not loaded: @rpath/libjpeg.9.dylib\n",
      "  Referenced from: <367D4265-B20F-34BD-94EB-4F3EE47C385B> /opt/anaconda3/envs/Python_interpre/lib/python3.12/site-packages/torchvision/image.so\n",
      "  Reason: tried: '/opt/anaconda3/envs/Python_interpre/lib/python3.12/site-packages/torchvision/../../../libjpeg.9.dylib' (no such file), '/opt/anaconda3/envs/Python_interpre/lib/python3.12/site-packages/torchvision/../../../libjpeg.9.dylib' (no such file), '/opt/anaconda3/envs/Python_interpre/lib/python3.12/lib-dynload/../../libjpeg.9.dylib' (no such file), '/opt/anaconda3/envs/Python_interpre/bin/../lib/libjpeg.9.dylib' (no such file)'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Concrete MethodModule class for a specific learning MethodModule\n",
    "'''\n",
    "\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers.models.bert.modeling_bert import BertPredictionHeadTransform, BertAttention, BertIntermediate, BertOutput\n",
    "from transformers.configuration_utils import PretrainedConfig\n",
    "\n",
    "BertLayerNorm = torch.nn.LayerNorm\n",
    "\n",
    "class BertEncoder(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(BertEncoder, self).__init__()\n",
    "        self.output_attentions = config.output_attentions\n",
    "        self.output_hidden_states = config.output_hidden_states\n",
    "        self.layer = nn.ModuleList([BertLayer(config) for _ in range(config.num_hidden_layers)])\n",
    "\n",
    "    def forward(self, hidden_states, attention_mask=None, head_mask=None, encoder_hidden_states=None, encoder_attention_mask=None, residual_h=None):\n",
    "        all_hidden_states = ()\n",
    "        all_attentions = ()\n",
    "        for i, layer_module in enumerate(self.layer):\n",
    "            if self.output_hidden_states:\n",
    "                all_hidden_states = all_hidden_states + (hidden_states,)\n",
    "\n",
    "            layer_outputs = layer_module(hidden_states, attention_mask, head_mask[i], encoder_hidden_states, encoder_attention_mask)\n",
    "            hidden_states = layer_outputs[0]\n",
    "\n",
    "            #---- add residual ----\n",
    "            if residual_h is not None:\n",
    "                for index in range(hidden_states.size()[1]):\n",
    "                    hidden_states[:,index,:] += residual_h\n",
    "\n",
    "            if self.output_attentions:\n",
    "                all_attentions = all_attentions + (layer_outputs[1],)\n",
    "\n",
    "        # Add last layer\n",
    "        if self.output_hidden_states:\n",
    "            all_hidden_states = all_hidden_states + (hidden_states,)\n",
    "\n",
    "        outputs = (hidden_states,)\n",
    "        if self.output_hidden_states:\n",
    "            outputs = outputs + (all_hidden_states,)\n",
    "        if self.output_attentions:\n",
    "            outputs = outputs + (all_attentions,)\n",
    "        return outputs  # last-layer hidden state, (all hidden states), (all attentions)\n",
    "\n",
    "\n",
    "class BertEmbeddings(nn.Module):\n",
    "    \"\"\"Construct the embeddings from features, wl, position and hop vectors.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super(BertEmbeddings, self).__init__()\n",
    "        self.raw_feature_embeddings = nn.Linear(config.x_size, config.hidden_size)\n",
    "        self.wl_role_embeddings = nn.Embedding(config.max_wl_role_index, config.hidden_size)\n",
    "        self.inti_pos_embeddings = nn.Embedding(config.max_inti_pos_index, config.hidden_size)\n",
    "        self.hop_dis_embeddings = nn.Embedding(config.max_hop_dis_index, config.hidden_size)\n",
    "        # self.attr_dis_embeddings = nn.Linear(1, config.hidden_size)\n",
    "\n",
    "        self.LayerNorm = BertLayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "\n",
    "    def forward(self, raw_features=None, wl_role_ids=None, init_pos_ids=None, hop_dis_ids=None, attr_ids=None):\n",
    "\n",
    "        raw_feature_embeds = self.raw_feature_embeddings(raw_features)\n",
    "        # raw_feature_embeds_drop = self.dropout(raw_feature_embeds)\n",
    "        \n",
    "        role_embeddings = self.wl_role_embeddings(wl_role_ids)\n",
    "        # role_embeddings_drop = self.dropout(role_embeddings)\n",
    "        \n",
    "        position_embeddings = self.inti_pos_embeddings(init_pos_ids)\n",
    "        # position_embeddings_drop = self.dropout(position_embeddings)\n",
    "        \n",
    "        hop_embeddings = self.hop_dis_embeddings(hop_dis_ids)\n",
    "        # hop_embeddings_drop = self.dropout(hop_embeddings)\n",
    "        \n",
    "        # attr_embeddings = self.attr_dis_embeddings(attr_ids.unsqueeze(-1))\n",
    "        # attr_embeddings_drop = self.dropout(attr_embeddings)\n",
    "\n",
    "        #---- here, we use summation ----\n",
    "        # embeddings = raw_feature_embeds + role_embeddings + position_embeddings + hop_embeddings + attr_embeddings\n",
    "        embeddings = raw_feature_embeds + role_embeddings + position_embeddings + hop_embeddings\n",
    "        # embeddings = raw_feature_embeds_drop + role_embeddings_drop + position_embeddings_drop + hop_embeddings_drop + attr_embeddings_drop\n",
    "        embeddings = self.LayerNorm(embeddings)\n",
    "        embeddings = self.dropout(embeddings)\n",
    "        return embeddings\n",
    "\n",
    "class NodeConstructOutputLayer(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(NodeConstructOutputLayer, self).__init__()\n",
    "        self.transform = BertPredictionHeadTransform(config)\n",
    "\n",
    "        # The output weights are the same as the input embeddings, but there is\n",
    "        # an output-only bias for each token.\n",
    "        self.decoder = nn.Linear(config.hidden_size, config.x_size, bias=False)\n",
    "\n",
    "        self.bias = nn.Parameter(torch.zeros(config.x_size))\n",
    "\n",
    "        # Need a link between the two variables so that the bias is correctly resized with `resize_token_embeddings`\n",
    "        self.decoder.bias = self.bias\n",
    "\n",
    "    def forward(self, hidden_states):\n",
    "        hidden_states = self.transform(hidden_states)\n",
    "        hidden_states = self.decoder(hidden_states) + self.bias\n",
    "        return hidden_states\n",
    "\n",
    "class BertLayer(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.attention = BertAttention(config)\n",
    "        self.is_decoder = config.is_decoder\n",
    "        if self.is_decoder:\n",
    "            self.crossattention = BertAttention(config)\n",
    "        self.intermediate = BertIntermediate(config)\n",
    "        self.output = BertOutput(config)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        hidden_states,\n",
    "        attention_mask=None,\n",
    "        head_mask=None,\n",
    "        encoder_hidden_states=None,\n",
    "        encoder_attention_mask=None,\n",
    "    ):\n",
    "        self_attention_outputs = self.attention(hidden_states, attention_mask, head_mask)\n",
    "        attention_output = self_attention_outputs[0]\n",
    "        outputs = self_attention_outputs[1:]  # add self attentions if we output attention weights\n",
    "\n",
    "        if self.is_decoder and encoder_hidden_states is not None:\n",
    "            cross_attention_outputs = self.crossattention(\n",
    "                attention_output, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask\n",
    "            )\n",
    "            attention_output = cross_attention_outputs[0]\n",
    "            outputs = outputs + cross_attention_outputs[1:]  # add cross attentions if we output attention weights\n",
    "\n",
    "        intermediate_output = self.intermediate(attention_output)\n",
    "        layer_output = self.output(intermediate_output, attention_output)\n",
    "        outputs = (layer_output,) + outputs\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4af62057",
   "metadata": {},
   "source": [
    "# Graph Bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a1003be4",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Concrete MethodModule class for a specific learning MethodModule\n",
    "'''\n",
    "\n",
    "import torch\n",
    "from transformers.models.bert.modeling_bert import BertPreTrainedModel, BertPooler\n",
    "\n",
    "BertLayerNorm = torch.nn.LayerNorm\n",
    "\n",
    "class MethodGraphBert(BertPreTrainedModel):\n",
    "    data = None\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super(MethodGraphBert, self).__init__(config)\n",
    "        self.config = config\n",
    "\n",
    "        self.embeddings = BertEmbeddings(config)\n",
    "        self.encoder = BertEncoder(config)\n",
    "        self.pooler = BertPooler(config)\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "\n",
    "    def forward(self, raw_features, wl_role_ids, init_pos_ids, hop_dis_ids, head_mask=None, residual_h=None):\n",
    "        if head_mask is None:\n",
    "            head_mask = [None] * self.config.num_hidden_layers\n",
    "\n",
    "        embedding_output = self.embeddings(raw_features=raw_features, wl_role_ids=wl_role_ids, init_pos_ids=init_pos_ids, \n",
    "                                        hop_dis_ids=hop_dis_ids)\n",
    "        encoder_outputs = self.encoder(embedding_output, head_mask=head_mask, residual_h=residual_h)\n",
    "        sequence_output = encoder_outputs[0]\n",
    "        pooled_output = self.pooler(sequence_output)\n",
    "        outputs = (sequence_output, pooled_output,) + encoder_outputs[1:]\n",
    "        \n",
    "        # return outputs, embedding_output, encoder_outputs, sequence_output, pooled_output # Test Output \n",
    "        return outputs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e98c0c1",
   "metadata": {},
   "source": [
    "# GraphBert Initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4401371c",
   "metadata": {},
   "source": [
    "# Params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2227cf68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cora config\n",
    "nclass = len(data.y.unique()) # for cora dataset\n",
    "nfeature = data.x.shape[1]\n",
    "ngraph = data.x.shape[0]\n",
    "\n",
    "#\n",
    "x_size = nfeature\n",
    "y_size = nclass\n",
    "graph_size = ngraph\n",
    "residual_type = 'graph_raw'\n",
    "# residual_type = 'raw'\n",
    "# residual_type = 'none'## good performances\n",
    "\n",
    "#Bert Config\n",
    "max_wl_role_index = 100\n",
    "max_hop_dis_index = 100\n",
    "max_attr_dis_index = embedding_dimension+1\n",
    "max_inti_pos_index = 100\n",
    "residual_type = residual_type\n",
    "x_size = x_size\n",
    "y_size = y_size\n",
    "k = nclass#Embedding dimension\n",
    "\n",
    "\n",
    "# Network setting\n",
    "hidden_size = 32 #32\n",
    "num_hidden_layers = 1 #2\n",
    "num_attention_heads = 4 #2\n",
    "hidden_act = 'gelu'\n",
    "intermediate_size = 128 #32: 2*hidden_size\n",
    "hidden_dropout_prob = 0.2#0.5\n",
    "attention_probs_dropout_prob = 0.2#0.3\n",
    "initializer_range = 0.02\n",
    "layer_norm_eps = 1e-12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4f800ca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    pass  # simple container\n",
    "\n",
    "config = Config()\n",
    "\n",
    "# Input embedding\n",
    "config.x_size = x_size\n",
    "config.hidden_size = hidden_size\n",
    "config.max_wl_role_index = max_wl_role_index\n",
    "config.max_inti_pos_index = max_inti_pos_index\n",
    "config.max_hop_dis_index = max_hop_dis_index\n",
    "# config.max_attr_dis_index = max_attr_dis_index\n",
    "config.layer_norm_eps = layer_norm_eps\n",
    "config.hidden_dropout_prob = hidden_dropout_prob\n",
    "\n",
    "# Encoder\n",
    "config.output_attentions = False\n",
    "config.output_hidden_states = False\n",
    "config.num_hidden_layers = num_hidden_layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a82d42f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphBertConfig(PretrainedConfig):\n",
    "    # default values\n",
    "    def __init__(\n",
    "        self,\n",
    "        residual_type = 'none',\n",
    "        x_size=3000,\n",
    "        y_size=7,\n",
    "        k=5,\n",
    "        max_wl_role_index = 100,\n",
    "        max_hop_dis_index = 100,\n",
    "        max_inti_pos_index = 100,\n",
    "        # max_attr_dis_index = 100,\n",
    "        hidden_size=32,#32,\n",
    "        num_hidden_layers=1,\n",
    "        num_attention_heads=1,\n",
    "        intermediate_size=32,#32,\n",
    "        hidden_act=\"gelu\",\n",
    "        hidden_dropout_prob=0.5,#0.5,\n",
    "        attention_probs_dropout_prob=0.5,#0.3,\n",
    "        initializer_range=0.02,\n",
    "        layer_norm_eps=1e-12,\n",
    "        is_decoder=False,\n",
    "        input_similarity = None,\n",
    "        input_feature = None,\n",
    "        **kwargs\n",
    "    ):\n",
    "        super(GraphBertConfig, self).__init__(**kwargs)\n",
    "        self.max_wl_role_index = max_wl_role_index\n",
    "        self.max_hop_dis_index = max_hop_dis_index\n",
    "        self.max_inti_pos_index = max_inti_pos_index\n",
    "        # self.max_attr_dis_index = max_attr_dis_index\n",
    "        self.residual_type = residual_type\n",
    "        self.x_size = x_size\n",
    "        self.y_size = y_size\n",
    "        self.k = k\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_hidden_layers = num_hidden_layers\n",
    "        self.num_attention_heads = num_attention_heads\n",
    "        self.hidden_act = hidden_act\n",
    "        self.intermediate_size = intermediate_size\n",
    "        self.hidden_dropout_prob = hidden_dropout_prob\n",
    "        self.attention_probs_dropout_prob = attention_probs_dropout_prob\n",
    "        self.initializer_range = initializer_range\n",
    "        self.layer_norm_eps = layer_norm_eps\n",
    "        self.is_decoder = is_decoder\n",
    "        self.input_similarity = input_similarity\n",
    "        self.input_feature = input_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b11f988d",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_config = GraphBertConfig(residual_type = residual_type, k=k, x_size=nfeature, y_size=y_size, \\\n",
    "                            hidden_size=hidden_size, intermediate_size=intermediate_size, \\\n",
    "                            num_attention_heads=num_attention_heads, \\\n",
    "                            num_hidden_layers=num_hidden_layers, \\\n",
    "                            input_similarity = data.adj, \\\n",
    "                            input_feature = data.x \n",
    "                            )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4005504",
   "metadata": {},
   "source": [
    "# Pre-training: Node reconstruction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8d41cbef",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MethodGraphBertNodeConstruct(BertPreTrainedModel):\n",
    "    learning_record_dict = {}\n",
    "    lr = 0.0001\n",
    "    weight_decay = 5e-4\n",
    "    max_epoch = 200\n",
    "    load_pretrained_path = ''\n",
    "    save_pretrained_path = ''\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super(MethodGraphBertNodeConstruct, self).__init__(config)\n",
    "        self.config = config\n",
    "        self.bert = MethodGraphBert(config)\n",
    "        self.cls_y = torch.nn.Linear(config.hidden_size, config.x_size)\n",
    "        self.init_weights()\n",
    "\n",
    "    def forward(self, raw_features, wl_role_ids, init_pos_ids, hop_dis_ids, idx=None):\n",
    "\n",
    "        outputs = self.bert(raw_features, wl_role_ids, init_pos_ids, hop_dis_ids)\n",
    "\n",
    "        sequence_output = 0\n",
    "        for i in range(self.config.k+1):\n",
    "            sequence_output += outputs[0][:,i,:]\n",
    "        sequence_output /= float(self.config.k+1)\n",
    "\n",
    "        x_hat = self.cls_y(sequence_output)\n",
    "\n",
    "        return x_hat\n",
    "    \n",
    "GraphBertNodeConstruct = MethodGraphBertNodeConstruct(bert_config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "64782593",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 loss_train: 0.0151 time: 0.1879s\n",
      "Epoch: 0011 loss_train: 0.0129 time: 0.1313s\n",
      "Epoch: 0021 loss_train: 0.0122 time: 0.1526s\n",
      "Epoch: 0031 loss_train: 0.0119 time: 0.1444s\n",
      "Epoch: 0041 loss_train: 0.0117 time: 0.1552s\n",
      "Epoch: 0051 loss_train: 0.0115 time: 0.1332s\n",
      "Epoch: 0061 loss_train: 0.0114 time: 0.1373s\n",
      "Epoch: 0071 loss_train: 0.0114 time: 0.1439s\n",
      "Epoch: 0081 loss_train: 0.0113 time: 0.1487s\n",
      "Epoch: 0091 loss_train: 0.0112 time: 0.1360s\n",
      "Epoch: 0101 loss_train: 0.0112 time: 0.1351s\n",
      "Epoch: 0111 loss_train: 0.0112 time: 0.1385s\n",
      "Epoch: 0121 loss_train: 0.0111 time: 0.1433s\n",
      "Epoch: 0131 loss_train: 0.0111 time: 0.1416s\n",
      "Epoch: 0141 loss_train: 0.0111 time: 0.1451s\n",
      "Epoch: 0151 loss_train: 0.0111 time: 0.1590s\n",
      "Epoch: 0161 loss_train: 0.0110 time: 0.1429s\n",
      "Epoch: 0171 loss_train: 0.0110 time: 0.1357s\n",
      "Epoch: 0181 loss_train: 0.0110 time: 0.1385s\n",
      "Epoch: 0191 loss_train: 0.0110 time: 0.1340s\n",
      "Optimization Finished!\n",
      "Total time elapsed: 29.0451s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "29.04515790939331"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "import time\n",
    "\n",
    "lr = 0.001\n",
    "weight_decay = 0.0005\n",
    "max_epoch = 200\n",
    "\n",
    "node_learning_record_dict = {}\n",
    "\n",
    "t_begin = time.time()\n",
    "optimizer = optim.AdamW(GraphBertNodeConstruct.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "for epoch in range(max_epoch):\n",
    "    t_epoch_begin = time.time()\n",
    "\n",
    "    # -------------------------\n",
    "\n",
    "    GraphBertNodeConstruct.train()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    output = GraphBertNodeConstruct.forward(raw_embeddings, wl_embedding, int_embeddings, hop_embeddings)\n",
    "\n",
    "    # loss_train = F.mse_loss(output, data.x)\n",
    "    loss_train = F.mse_loss(output, data.x)\n",
    "\n",
    "    loss_train.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    node_learning_record_dict[epoch] = {'loss_train': loss_train.item(), 'time': time.time() - t_epoch_begin}\n",
    "\n",
    "    # -------------------------\n",
    "    if epoch % 10 == 0:\n",
    "        print('Epoch: {:04d}'.format(epoch + 1),\n",
    "                'loss_train: {:.4f}'.format(loss_train.item()),\n",
    "                'time: {:.4f}s'.format(time.time() - t_epoch_begin))\n",
    "\n",
    "print(\"Optimization Finished!\")\n",
    "print(\"Total time elapsed: {:.4f}s\".format(time.time() - t_begin))\n",
    "time.time() - t_begin"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40eb0c8a",
   "metadata": {},
   "source": [
    "# Fine-Tuning for the real world"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fcba3002",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from transformers.models.bert.modeling_bert import BertPreTrainedModel\n",
    "\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "import abc\n",
    "\n",
    "\n",
    "class evaluate:\n",
    "    \"\"\" \n",
    "    evaluate: Abstract Class\n",
    "    Entries: \n",
    "    \"\"\"\n",
    "    \n",
    "    evaluate_name = None\n",
    "    evaluate_description = None\n",
    "    \n",
    "    data = None\n",
    "    \n",
    "    # initialization function\n",
    "    def __init__(self, eName=None, eDescription=None):\n",
    "        self.evaluate_name = eName\n",
    "        self.evaluate_description = eDescription\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def evaluate(self):\n",
    "        return\n",
    "\n",
    "class EvaluateAcc(evaluate):\n",
    "    data = None\n",
    "    \n",
    "    def evaluate(self):\n",
    "        \n",
    "        return accuracy_score(self.data['true_y'], self.data['pred_y'])\n",
    "\n",
    "BertLayerNorm = torch.nn.LayerNorm\n",
    "\n",
    "class MethodGraphBertNodeClassification(BertPreTrainedModel):\n",
    "    learning_record_dict = {}\n",
    "    lr = 0.001\n",
    "    weight_decay = 5e-4\n",
    "    max_epoch = 500\n",
    "    spy_tag = True\n",
    "\n",
    "    load_pretrained_path = ''\n",
    "    save_pretrained_path = ''\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super(MethodGraphBertNodeClassification, self).__init__(config)\n",
    "        self.config = config\n",
    "        self.bert = MethodGraphBert(config)\n",
    "        self.res_h = torch.nn.Linear(config.x_size, config.hidden_size)\n",
    "        self.res_y = torch.nn.Linear(config.x_size, config.y_size)\n",
    "        self.cls_y = torch.nn.Linear(config.hidden_size, config.y_size)\n",
    "\n",
    "        \n",
    "        self.init_weights()\n",
    "\n",
    "    def forward(self, raw_features, wl_role_ids, init_pos_ids, hop_dis_ids, data, idx=None):\n",
    "        \n",
    "        #Residual\n",
    "        residual_h, residual_y = self.residual_term()\n",
    "        \n",
    "        if idx is not None:\n",
    "            if residual_h is None:\n",
    "                outputs = self.bert(raw_features[idx], wl_role_ids[idx], init_pos_ids[idx], hop_dis_ids[idx], residual_h=None)\n",
    "            else:\n",
    "                outputs = self.bert(raw_features[idx], wl_role_ids[idx], init_pos_ids[idx], hop_dis_ids[idx], residual_h=residual_h[idx])\n",
    "                residual_y = residual_y[idx]\n",
    "            \n",
    "        else:\n",
    "            if residual_h is None:\n",
    "                outputs = self.bert(raw_features, wl_role_ids, init_pos_ids, hop_dis_ids, residual_h=None)\n",
    "            else:\n",
    "                outputs = self.bert(raw_features, wl_role_ids, init_pos_ids, hop_dis_ids, residual_h=residual_h)\n",
    "        \n",
    "        # Average the sequence output\n",
    "        sequence_output = 0\n",
    "        for i in range(self.config.k+1):\n",
    "            sequence_output += outputs[0][:,i,:]\n",
    "        sequence_output /= float(self.config.k+1)#opt1\n",
    "        # sequence_output = outputs[0].mean(dim=1)#opt2\n",
    "        \n",
    "        \n",
    "        labels = self.cls_y(sequence_output)\n",
    "\n",
    "        if residual_y is not None:\n",
    "            labels += residual_y\n",
    "\n",
    "        return F.log_softmax(labels, dim=1)\n",
    "        \n",
    "\n",
    "    def residual_term(self):\n",
    "        if self.config.residual_type == 'none':\n",
    "            return None, None\n",
    "        elif self.config.residual_type == 'raw':\n",
    "            return self.res_h(self.config.input_feature), self.res_y(self.config.input_feature)\n",
    "        elif self.config.residual_type == 'graph_raw':\n",
    "            return torch.spmm(self.config.input_similarity, self.res_h(self.config.input_feature)), torch.spmm(self.config.input_similarity, self.res_y(self.config.input_feature))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c1b4e77e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MethodGraphBertNodeClassification(\n",
      "  (bert): MethodGraphBert(\n",
      "    (embeddings): BertEmbeddings(\n",
      "      (raw_feature_embeddings): Linear(in_features=1433, out_features=32, bias=True)\n",
      "      (wl_role_embeddings): Embedding(100, 32)\n",
      "      (inti_pos_embeddings): Embedding(100, 32)\n",
      "      (hop_dis_embeddings): Embedding(100, 32)\n",
      "      (LayerNorm): LayerNorm((32,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.5, inplace=False)\n",
      "    )\n",
      "    (encoder): BertEncoder(\n",
      "      (layer): ModuleList(\n",
      "        (0): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSdpaSelfAttention(\n",
      "              (query): Linear(in_features=32, out_features=32, bias=True)\n",
      "              (key): Linear(in_features=32, out_features=32, bias=True)\n",
      "              (value): Linear(in_features=32, out_features=32, bias=True)\n",
      "              (dropout): Dropout(p=0.5, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=32, out_features=32, bias=True)\n",
      "              (LayerNorm): LayerNorm((32,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.5, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=32, out_features=128, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=128, out_features=32, bias=True)\n",
      "            (LayerNorm): LayerNorm((32,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.5, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (pooler): BertPooler(\n",
      "      (dense): Linear(in_features=32, out_features=32, bias=True)\n",
      "      (activation): Tanh()\n",
      "    )\n",
      "  )\n",
      "  (res_h): Linear(in_features=1433, out_features=32, bias=True)\n",
      "  (res_y): Linear(in_features=1433, out_features=7, bias=True)\n",
      "  (cls_y): Linear(in_features=32, out_features=7, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "GraphBertNodeClassification = MethodGraphBertNodeClassification(bert_config)\n",
    "print(GraphBertNodeClassification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a131bdc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score,\n",
    "    f1_score, roc_auc_score, confusion_matrix\n",
    ")\n",
    "\n",
    "# ------------------------------------\n",
    "# evaluation function\n",
    "def evaluate_in_batches(model, idx_eval, batch_size=10):\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        \n",
    "        epoch_loss = 0.0\n",
    "        epoch_correct = 0\n",
    "        epoch_total = 0\n",
    "        \n",
    "        \n",
    "        all_preds = []\n",
    "        all_labels = []\n",
    "        all_probs = []\n",
    "        \n",
    "        for idx in idx_eval:\n",
    "            \n",
    "            \n",
    "            label_ouputs = model.forward(raw_embeddings, wl_embedding,\n",
    "                                int_embeddings, hop_embeddings, data, idx)\n",
    "            \n",
    "                \n",
    "            loss_train_cls_residual = F.cross_entropy(label_ouputs, data.y[idx])\n",
    "        \n",
    "            epoch_loss += loss_train_cls_residual.item() * len(data.y[idx])# based on the number of samples\n",
    "            \n",
    "            \n",
    "            #### examine the metrics\n",
    "            probs = F.softmax(label_ouputs, dim=1)  # shape: (batch, 2)\n",
    "            preds = torch.argmax(label_ouputs, dim=1)  # predicted class\n",
    "            \n",
    "            \n",
    "            all_probs.append(probs.numpy())\n",
    "            all_preds.extend(preds.numpy())\n",
    "            all_labels.extend(data.y[idx].numpy())\n",
    "            \n",
    "            \n",
    "            epoch_correct += preds.eq(data.y[idx]).sum().item()\n",
    "            epoch_total += len(label_ouputs)\n",
    "            \n",
    "        \n",
    "        \n",
    "            \n",
    "        loss_train_avg = epoch_loss / epoch_total\n",
    "        acc_train_avg = epoch_correct / epoch_total\n",
    "        \n",
    "        # Calculate AUC\n",
    "        all_labels = np.array(all_labels)\n",
    "        all_preds = np.array(all_preds)\n",
    "        # all_probs = np.array(all_probs)\n",
    "        all_probs = np.concatenate(all_probs, axis=0)\n",
    "        \n",
    "        accuracy = accuracy_score(all_labels, all_preds)\n",
    "        precision = precision_score(all_labels, all_preds, average='macro')\n",
    "        recall = recall_score(all_labels, all_preds, average='macro')\n",
    "        f1 = f1_score(all_labels, all_preds, average='macro')\n",
    "        \n",
    "        \n",
    "        roc_auc = roc_auc_score(\n",
    "            all_labels, all_probs, multi_class='ovr')\n",
    "        \n",
    "        \n",
    "        cm = confusion_matrix(all_labels, all_preds).ravel()\n",
    "            \n",
    "    return acc_train_avg, loss_train_avg, accuracy, precision, recall, f1, roc_auc, cm\n",
    "\n",
    "# ------------------------------------\n",
    "# early stopping\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=10, delta=0.0, mode='min', path='checkpoint.pt'):\n",
    "        self.patience = patience\n",
    "        self.delta = delta\n",
    "        self.mode = mode\n",
    "        self.path = path\n",
    "\n",
    "        self.best_score = None\n",
    "        self.counter = 0\n",
    "        self.early_stop = False\n",
    "        self.best_epoch = None\n",
    "\n",
    "        if self.mode not in ['min', 'max']:\n",
    "            raise ValueError(\"mode must be 'min' or 'max'\")\n",
    "\n",
    "    def __call__(self, val_metric, model, epoch=None):\n",
    "        score = val_metric if self.mode == 'min' else -val_metric\n",
    "\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "            self.best_epoch = epoch\n",
    "            self.save_checkpoint(model, val_metric)\n",
    "        elif score < self.best_score - self.delta:  # ✅ only when truly improved\n",
    "            self.best_score = score\n",
    "            self.best_epoch = epoch\n",
    "            self.counter = 0\n",
    "            self.save_checkpoint(model, val_metric)\n",
    "        else:\n",
    "            self.counter += 1\n",
    "            print(f\"No improvement. EarlyStopping counter: {self.counter}/{self.patience}\")\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "\n",
    "    def save_checkpoint(self, model, val_metric):\n",
    "        torch.save(model.state_dict(), self.path)\n",
    "        print(f\"Model improved. Saved to {self.path} | Val Metric: {val_metric:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f126eba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001\n",
      "  [Train] loss: 1.8672 | acc: 0.2357\n",
      "  [Valid] loss: 1.6794 | acc: 0.3567\n",
      "  [Test ] loss: 1.5799 | acc: 0.4400\n",
      "  Time: 0.1659s\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/Python_interpre/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/opt/anaconda3/envs/Python_interpre/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Parent directory ./results. does not exist.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[25]\u001b[39m\u001b[32m, line 119\u001b[39m\n\u001b[32m    115\u001b[39m \u001b[38;5;66;03m#lr scheduler\u001b[39;00m\n\u001b[32m    116\u001b[39m scheduler.step(loss_val)\n\u001b[32m--> \u001b[39m\u001b[32m119\u001b[39m \u001b[43mearly_stopping\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mGraphBertNodeClassification\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    121\u001b[39m \u001b[38;5;66;03m# Early stopping\u001b[39;00m\n\u001b[32m    122\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m early_stopping.early_stop:\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[24]\u001b[39m\u001b[32m, line 98\u001b[39m, in \u001b[36mEarlyStopping.__call__\u001b[39m\u001b[34m(self, val_metric, model, epoch)\u001b[39m\n\u001b[32m     96\u001b[39m     \u001b[38;5;28mself\u001b[39m.best_score = score\n\u001b[32m     97\u001b[39m     \u001b[38;5;28mself\u001b[39m.best_epoch = epoch\n\u001b[32m---> \u001b[39m\u001b[32m98\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msave_checkpoint\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_metric\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     99\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m score < \u001b[38;5;28mself\u001b[39m.best_score - \u001b[38;5;28mself\u001b[39m.delta:  \u001b[38;5;66;03m# ✅ only when truly improved\u001b[39;00m\n\u001b[32m    100\u001b[39m     \u001b[38;5;28mself\u001b[39m.best_score = score\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[24]\u001b[39m\u001b[32m, line 111\u001b[39m, in \u001b[36mEarlyStopping.save_checkpoint\u001b[39m\u001b[34m(self, model, val_metric)\u001b[39m\n\u001b[32m    110\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34msave_checkpoint\u001b[39m(\u001b[38;5;28mself\u001b[39m, model, val_metric):\n\u001b[32m--> \u001b[39m\u001b[32m111\u001b[39m     \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    112\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mModel improved. Saved to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.path\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m | Val Metric: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mval_metric\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/Python_interpre/lib/python3.12/site-packages/torch/serialization.py:849\u001b[39m, in \u001b[36msave\u001b[39m\u001b[34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization, _disable_byteorder_record)\u001b[39m\n\u001b[32m    846\u001b[39m _check_save_filelike(f)\n\u001b[32m    848\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m _use_new_zipfile_serialization:\n\u001b[32m--> \u001b[39m\u001b[32m849\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43m_open_zipfile_writer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m opened_zipfile:\n\u001b[32m    850\u001b[39m         _save(\n\u001b[32m    851\u001b[39m             obj,\n\u001b[32m    852\u001b[39m             opened_zipfile,\n\u001b[32m   (...)\u001b[39m\u001b[32m    855\u001b[39m             _disable_byteorder_record,\n\u001b[32m    856\u001b[39m         )\n\u001b[32m    857\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/Python_interpre/lib/python3.12/site-packages/torch/serialization.py:716\u001b[39m, in \u001b[36m_open_zipfile_writer\u001b[39m\u001b[34m(name_or_buffer)\u001b[39m\n\u001b[32m    714\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    715\u001b[39m     container = _open_zipfile_writer_buffer\n\u001b[32m--> \u001b[39m\u001b[32m716\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcontainer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname_or_buffer\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/Python_interpre/lib/python3.12/site-packages/torch/serialization.py:687\u001b[39m, in \u001b[36m_open_zipfile_writer_file.__init__\u001b[39m\u001b[34m(self, name)\u001b[39m\n\u001b[32m    685\u001b[39m     \u001b[38;5;28msuper\u001b[39m().\u001b[34m__init__\u001b[39m(torch._C.PyTorchFileWriter(\u001b[38;5;28mself\u001b[39m.file_stream))\n\u001b[32m    686\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m687\u001b[39m     \u001b[38;5;28msuper\u001b[39m().\u001b[34m__init__\u001b[39m(\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_C\u001b[49m\u001b[43m.\u001b[49m\u001b[43mPyTorchFileWriter\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[31mRuntimeError\u001b[39m: Parent directory ./results. does not exist."
     ]
    }
   ],
   "source": [
    "# Separate dataset for purpose, we only use limited dataset 140 nodes for training to test relatively big dataset\n",
    "idx_train = range(140)\n",
    "idx_test = range(200, 1200)\n",
    "idx_val = range(1200, 1500)\n",
    "\n",
    "\n",
    "classify_learning_record_dict = {}\n",
    "\n",
    "\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "train_loader = DataLoader(idx_train, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(idx_test, batch_size=64, shuffle=True)\n",
    "val_loader = DataLoader(idx_val, batch_size=64, shuffle=True)\n",
    "\n",
    "\n",
    "# ------------------------------------\n",
    "params = []\n",
    "base_lr = 1e-3\n",
    "decay_factor = 0.9\n",
    "for i, (name, param) in enumerate(GraphBertNodeClassification.named_parameters()):\n",
    "    lr = base_lr * (decay_factor ** (len(list(GraphBertNodeClassification.named_children())) - i - 1))\n",
    "    params.append({'params': param, 'lr': lr})\n",
    "    \n",
    "# ------------------------------------\n",
    "max_epoch = 100\n",
    "\n",
    "\n",
    "t_begin = time.time()\n",
    "\n",
    "optimizer = optim.Adam(params, lr=base_lr, weight_decay=1e-4)\n",
    "\n",
    "accuracy = EvaluateAcc('', '')\n",
    "\n",
    "# initialization \n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=2, verbose=True)\n",
    "\n",
    "\n",
    "classify_learning_record_dict = {}\n",
    "\n",
    "\n",
    "# Early Stopping\n",
    "early_stopping = EarlyStopping(patience=30, mode='min', path=f'./results/checkpoint.pt')\n",
    "\n",
    "\n",
    "max_score = 0.0\n",
    "for epoch in range(max_epoch):\n",
    "    t_epoch_begin = time.time()\n",
    "\n",
    "    GraphBertNodeClassification.train()\n",
    "\n",
    "    epoch_loss = 0.0\n",
    "    epoch_correct = 0\n",
    "    epoch_total = 0\n",
    "    \n",
    "    for load in train_loader:\n",
    "\n",
    "        optimizer.zero_grad()            \n",
    "        \n",
    "        output  = GraphBertNodeClassification.forward(\n",
    "            raw_embeddings, wl_embedding, int_embeddings, hop_embeddings, data, idx=load)\n",
    "        \n",
    "        # Two loss functions\n",
    "        loss_train = F.cross_entropy(output, data.y[load])\n",
    "        \n",
    "        loss_train.backward()\n",
    "        \n",
    "        pred = (output).max(1)[1]\n",
    "        correct = pred.eq(data.y[load]).sum().item()\n",
    "\n",
    "        # epoch_loss += loss_train.item() * len(batch_idx)\n",
    "        epoch_loss += loss_train.item() * len(load)# based on the number of samples\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        \n",
    "        epoch_correct += correct\n",
    "        \n",
    "        # epoch_total += len(batch_idx)\n",
    "        epoch_total += len(load)\n",
    "        \n",
    "        \n",
    "        # print(f\"Batch {i+1}/{num_batches}, Loss: {loss_train.item():.4f}, Accuracy: {correct/len(batch_idx):.4f}\")\n",
    "        \n",
    "\n",
    "    loss_train_avg = epoch_loss / epoch_total\n",
    "    acc_train_avg = epoch_correct / epoch_total\n",
    "\n",
    "    # evaluation\n",
    "    GraphBertNodeClassification.eval()#frozen\n",
    "    \n",
    "    # Validate (using mini-batch)\n",
    "    acc_val, loss_val, accuracy_val, precision_val, recall_val, f1_val, roc_auc_val, confusion_matrix_val = evaluate_in_batches(GraphBertNodeClassification, val_loader, batch_size=20)\n",
    "\n",
    "    # Test (using mini-batch)\n",
    "    acc_test, loss_test, accuracy_test, precision_test, recall_test, f1_test, roc_auc_test, confusion_matrix_test = evaluate_in_batches(GraphBertNodeClassification, test_loader, batch_size=20)\n",
    "\n",
    "    classify_learning_record_dict[epoch] = {\n",
    "        'loss_train': loss_train_avg,\n",
    "        'acc_train': acc_train_avg,\n",
    "        'loss_val': loss_val,\n",
    "        'acc_val': acc_val,\n",
    "        'loss_test': loss_test,\n",
    "        'acc_test': acc_test,\n",
    "        'time': time.time() - t_epoch_begin\n",
    "    }\n",
    "\n",
    "    print(f\"Epoch: {epoch+1:04d}\")\n",
    "    print(f\"  [Train] loss: {loss_train_avg:.4f} | acc: {acc_train_avg:.4f}\")\n",
    "    print(f\"  [Valid] loss: {loss_val:.4f} | acc: {acc_val:.4f}\")\n",
    "    print(f\"  [Test ] loss: {loss_test:.4f} | acc: {acc_test:.4f}\")\n",
    "    print(f\"  Time: {time.time() - t_epoch_begin:.4f}s\")\n",
    "    print('-----------------------------------------')\n",
    "    \n",
    "    #lr scheduler\n",
    "    scheduler.step(loss_val)\n",
    "    \n",
    "\n",
    "    early_stopping(loss_val, GraphBertNodeClassification)\n",
    "    \n",
    "    # Early stopping\n",
    "    if early_stopping.early_stop:\n",
    "        print(\"Early stopping triggered.\")\n",
    "        break\n",
    "\n",
    "\n",
    "print(\"Optimization Finished!\")\n",
    "print(\"Total time elapsed: {:.4f}s\".format(time.time() - t_begin) + ', \\\n",
    "    best testing performance {: 4f}'.format(np.max([classify_learning_record_dict[epoch]['acc_test'] for epoch in classify_learning_record_dict])) \\\n",
    "        + ', minimun loss {: 4f}'.format(np.min([classify_learning_record_dict[epoch]['loss_test'] for epoch in classify_learning_record_dict])))\n",
    "\n",
    "\n",
    "\n",
    "torch.save(classify_learning_record_dict, f'./results/classify_learning_record_dict.pth')\n",
    "    \n",
    "torch.save(GraphBertNodeClassification.state_dict(), './results/model_dict.pt')\n",
    "print(f\"Model saved\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python_interpre",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
